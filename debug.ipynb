{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2174b27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9261c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"dataset/t2m_train.npy\", allow_pickle=True).item()\n",
    "mean = np.load(\"dataset/t2m_mean.npy\")\n",
    "std = np.load(\"dataset/t2m_std.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37c15cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.96153536e-04 -7.36963557e-05  6.27978472e-04 ...  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00]\n",
      " [ 2.96227750e-04  2.62251106e-05  3.48566275e-04 ...  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00]\n",
      " [ 2.96163838e-04  1.87605925e-04 -1.74340166e-04 ...  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00]\n",
      " ...\n",
      " [ 1.15424395e-04  3.39752965e-04 -1.12489935e-04 ...  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00]\n",
      " [ 1.04760751e-04  3.78022582e-04  1.95059954e-04 ...  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00]\n",
      " [ 9.38493758e-05  3.08734307e-04  6.46549684e-04 ...  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00]]\n",
      "[{'caption': 'individual appears to be in a constant flrx.', 'tokens': ['individual/ADJ', 'appear/VERB', 'to/PART', 'be/AUX', 'in/ADP', 'a/DET', 'constant/ADJ', 'flrx/NOUN']}]\n",
      "[[  0.58640227  -0.07332673 -11.90158569 ...   9.88045141  10.74045868\n",
      "    9.85708595]\n",
      " [  0.58654647   0.07428338 -12.3143505  ...   9.88045141  10.74045868\n",
      "    9.85708595]\n",
      " [  0.58642229   0.31268501 -13.08681992 ...   9.88045141  10.74045868\n",
      "    9.85708595]\n",
      " ...\n",
      " [  0.2352492    0.53744593 -12.99545097 ...   9.88045141  10.74045868\n",
      "    9.85708595]\n",
      " [  0.21452996   0.59398016 -12.54111944 ...   9.88045141  10.74045868\n",
      "    9.85708595]\n",
      " [  0.19332938   0.49162327 -11.87415115 ...   9.88045141  10.74045868\n",
      "    9.85708595]]\n",
      "-5.652112106804473e-06\n",
      "0.0005146733976555149\n"
     ]
    }
   ],
   "source": [
    "name = data[\"name_list\"][0]\n",
    "motion = data[\"data_dict\"][name][\"motion\"]\n",
    "text = data[\"data_dict\"][name][\"text\"]\n",
    "\n",
    "\n",
    "\n",
    "print(motion)\n",
    "print(text)\n",
    "\n",
    "normalized_motion = (motion - mean) / std\n",
    "print(normalized_motion)\n",
    "\n",
    "print(mean[0])\n",
    "print(std[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1553857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.652112106804473e-06\n",
      "0.0005146733976555149\n"
     ]
    }
   ],
   "source": [
    "mean_original = np.load(\"/home/mahatano/workspace/t2m_mean.npy\")\n",
    "std_original = np.load(\"/home/mahatano/workspace/t2m_std.npy\")\n",
    "print(mean_original[0])\n",
    "print(std_original[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bfc96f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.652112106804473e-06\n",
      "0.012866834941387873\n"
     ]
    }
   ],
   "source": [
    "Mean = np.load(\"/home/mahatano/workspace/mdm/dataset/HumanML3D/Mean.npy\")\n",
    "Std = np.load(\"/home/mahatano/workspace/mdm/dataset/HumanML3D/Std.npy\")\n",
    "print(Mean[0])\n",
    "print(Std[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4950196a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n",
      "820\n",
      "model.input_process.poseEmbedding.weight torch.Size([512, 263])\n",
      "model.input_process.poseEmbedding.bias torch.Size([512])\n",
      "model.sequence_pos_encoder.pe torch.Size([5000, 1, 512])\n",
      "model.embed_timestep.sequence_pos_encoder.pe torch.Size([5000, 1, 512])\n",
      "model.embed_timestep.time_embed.0.weight torch.Size([512, 512])\n",
      "model.embed_timestep.time_embed.0.bias torch.Size([512])\n",
      "model.embed_timestep.time_embed.2.weight torch.Size([512, 512])\n",
      "model.embed_timestep.time_embed.2.bias torch.Size([512])\n",
      "model.clip_model.positional_embedding torch.Size([77, 512])\n",
      "model.clip_model.text_projection torch.Size([512, 512])\n",
      "model.clip_model.logit_scale torch.Size([])\n",
      "model.clip_model.visual.class_embedding torch.Size([768])\n",
      "model.clip_model.visual.positional_embedding torch.Size([50, 768])\n",
      "model.clip_model.visual.proj torch.Size([768, 512])\n",
      "model.clip_model.visual.conv1.weight torch.Size([768, 3, 32, 32])\n",
      "model.clip_model.visual.ln_pre.weight torch.Size([768])\n",
      "model.clip_model.visual.ln_pre.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.0.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model.clip_model.visual.transformer.resblocks.0.attn.in_proj_bias torch.Size([2304])\n",
      "model.clip_model.visual.transformer.resblocks.0.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.clip_model.visual.transformer.resblocks.0.attn.out_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.0.ln_1.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.0.ln_1.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.0.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model.clip_model.visual.transformer.resblocks.0.mlp.c_fc.bias torch.Size([3072])\n",
      "model.clip_model.visual.transformer.resblocks.0.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model.clip_model.visual.transformer.resblocks.0.mlp.c_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.0.ln_2.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.0.ln_2.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.1.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model.clip_model.visual.transformer.resblocks.1.attn.in_proj_bias torch.Size([2304])\n",
      "model.clip_model.visual.transformer.resblocks.1.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.clip_model.visual.transformer.resblocks.1.attn.out_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.1.ln_1.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.1.ln_1.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.1.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model.clip_model.visual.transformer.resblocks.1.mlp.c_fc.bias torch.Size([3072])\n",
      "model.clip_model.visual.transformer.resblocks.1.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model.clip_model.visual.transformer.resblocks.1.mlp.c_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.1.ln_2.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.1.ln_2.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.2.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model.clip_model.visual.transformer.resblocks.2.attn.in_proj_bias torch.Size([2304])\n",
      "model.clip_model.visual.transformer.resblocks.2.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.clip_model.visual.transformer.resblocks.2.attn.out_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.2.ln_1.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.2.ln_1.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.2.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model.clip_model.visual.transformer.resblocks.2.mlp.c_fc.bias torch.Size([3072])\n",
      "model.clip_model.visual.transformer.resblocks.2.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model.clip_model.visual.transformer.resblocks.2.mlp.c_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.2.ln_2.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.2.ln_2.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.3.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model.clip_model.visual.transformer.resblocks.3.attn.in_proj_bias torch.Size([2304])\n",
      "model.clip_model.visual.transformer.resblocks.3.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.clip_model.visual.transformer.resblocks.3.attn.out_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.3.ln_1.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.3.ln_1.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.3.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model.clip_model.visual.transformer.resblocks.3.mlp.c_fc.bias torch.Size([3072])\n",
      "model.clip_model.visual.transformer.resblocks.3.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model.clip_model.visual.transformer.resblocks.3.mlp.c_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.3.ln_2.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.3.ln_2.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.4.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model.clip_model.visual.transformer.resblocks.4.attn.in_proj_bias torch.Size([2304])\n",
      "model.clip_model.visual.transformer.resblocks.4.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.clip_model.visual.transformer.resblocks.4.attn.out_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.4.ln_1.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.4.ln_1.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.4.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model.clip_model.visual.transformer.resblocks.4.mlp.c_fc.bias torch.Size([3072])\n",
      "model.clip_model.visual.transformer.resblocks.4.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model.clip_model.visual.transformer.resblocks.4.mlp.c_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.4.ln_2.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.4.ln_2.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.5.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model.clip_model.visual.transformer.resblocks.5.attn.in_proj_bias torch.Size([2304])\n",
      "model.clip_model.visual.transformer.resblocks.5.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.clip_model.visual.transformer.resblocks.5.attn.out_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.5.ln_1.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.5.ln_1.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.5.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model.clip_model.visual.transformer.resblocks.5.mlp.c_fc.bias torch.Size([3072])\n",
      "model.clip_model.visual.transformer.resblocks.5.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model.clip_model.visual.transformer.resblocks.5.mlp.c_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.5.ln_2.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.5.ln_2.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.6.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model.clip_model.visual.transformer.resblocks.6.attn.in_proj_bias torch.Size([2304])\n",
      "model.clip_model.visual.transformer.resblocks.6.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.clip_model.visual.transformer.resblocks.6.attn.out_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.6.ln_1.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.6.ln_1.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.6.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model.clip_model.visual.transformer.resblocks.6.mlp.c_fc.bias torch.Size([3072])\n",
      "model.clip_model.visual.transformer.resblocks.6.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model.clip_model.visual.transformer.resblocks.6.mlp.c_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.6.ln_2.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.6.ln_2.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.7.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model.clip_model.visual.transformer.resblocks.7.attn.in_proj_bias torch.Size([2304])\n",
      "model.clip_model.visual.transformer.resblocks.7.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.clip_model.visual.transformer.resblocks.7.attn.out_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.7.ln_1.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.7.ln_1.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.7.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model.clip_model.visual.transformer.resblocks.7.mlp.c_fc.bias torch.Size([3072])\n",
      "model.clip_model.visual.transformer.resblocks.7.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model.clip_model.visual.transformer.resblocks.7.mlp.c_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.7.ln_2.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.7.ln_2.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.8.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model.clip_model.visual.transformer.resblocks.8.attn.in_proj_bias torch.Size([2304])\n",
      "model.clip_model.visual.transformer.resblocks.8.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.clip_model.visual.transformer.resblocks.8.attn.out_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.8.ln_1.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.8.ln_1.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.8.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model.clip_model.visual.transformer.resblocks.8.mlp.c_fc.bias torch.Size([3072])\n",
      "model.clip_model.visual.transformer.resblocks.8.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model.clip_model.visual.transformer.resblocks.8.mlp.c_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.8.ln_2.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.8.ln_2.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.9.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model.clip_model.visual.transformer.resblocks.9.attn.in_proj_bias torch.Size([2304])\n",
      "model.clip_model.visual.transformer.resblocks.9.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.clip_model.visual.transformer.resblocks.9.attn.out_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.9.ln_1.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.9.ln_1.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.9.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model.clip_model.visual.transformer.resblocks.9.mlp.c_fc.bias torch.Size([3072])\n",
      "model.clip_model.visual.transformer.resblocks.9.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model.clip_model.visual.transformer.resblocks.9.mlp.c_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.9.ln_2.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.9.ln_2.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.10.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model.clip_model.visual.transformer.resblocks.10.attn.in_proj_bias torch.Size([2304])\n",
      "model.clip_model.visual.transformer.resblocks.10.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.clip_model.visual.transformer.resblocks.10.attn.out_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.10.ln_1.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.10.ln_1.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.10.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model.clip_model.visual.transformer.resblocks.10.mlp.c_fc.bias torch.Size([3072])\n",
      "model.clip_model.visual.transformer.resblocks.10.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model.clip_model.visual.transformer.resblocks.10.mlp.c_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.10.ln_2.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.10.ln_2.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.11.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model.clip_model.visual.transformer.resblocks.11.attn.in_proj_bias torch.Size([2304])\n",
      "model.clip_model.visual.transformer.resblocks.11.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.clip_model.visual.transformer.resblocks.11.attn.out_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.11.ln_1.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.11.ln_1.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.11.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model.clip_model.visual.transformer.resblocks.11.mlp.c_fc.bias torch.Size([3072])\n",
      "model.clip_model.visual.transformer.resblocks.11.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model.clip_model.visual.transformer.resblocks.11.mlp.c_proj.bias torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.11.ln_2.weight torch.Size([768])\n",
      "model.clip_model.visual.transformer.resblocks.11.ln_2.bias torch.Size([768])\n",
      "model.clip_model.visual.ln_post.weight torch.Size([768])\n",
      "model.clip_model.visual.ln_post.bias torch.Size([768])\n",
      "model.clip_model.transformer.resblocks.0.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model.clip_model.transformer.resblocks.0.attn.in_proj_bias torch.Size([1536])\n",
      "model.clip_model.transformer.resblocks.0.attn.out_proj.weight torch.Size([512, 512])\n",
      "model.clip_model.transformer.resblocks.0.attn.out_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.0.ln_1.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.0.ln_1.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.0.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model.clip_model.transformer.resblocks.0.mlp.c_fc.bias torch.Size([2048])\n",
      "model.clip_model.transformer.resblocks.0.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model.clip_model.transformer.resblocks.0.mlp.c_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.0.ln_2.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.0.ln_2.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.1.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model.clip_model.transformer.resblocks.1.attn.in_proj_bias torch.Size([1536])\n",
      "model.clip_model.transformer.resblocks.1.attn.out_proj.weight torch.Size([512, 512])\n",
      "model.clip_model.transformer.resblocks.1.attn.out_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.1.ln_1.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.1.ln_1.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.1.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model.clip_model.transformer.resblocks.1.mlp.c_fc.bias torch.Size([2048])\n",
      "model.clip_model.transformer.resblocks.1.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model.clip_model.transformer.resblocks.1.mlp.c_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.1.ln_2.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.1.ln_2.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.2.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model.clip_model.transformer.resblocks.2.attn.in_proj_bias torch.Size([1536])\n",
      "model.clip_model.transformer.resblocks.2.attn.out_proj.weight torch.Size([512, 512])\n",
      "model.clip_model.transformer.resblocks.2.attn.out_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.2.ln_1.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.2.ln_1.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.2.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model.clip_model.transformer.resblocks.2.mlp.c_fc.bias torch.Size([2048])\n",
      "model.clip_model.transformer.resblocks.2.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model.clip_model.transformer.resblocks.2.mlp.c_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.2.ln_2.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.2.ln_2.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.3.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model.clip_model.transformer.resblocks.3.attn.in_proj_bias torch.Size([1536])\n",
      "model.clip_model.transformer.resblocks.3.attn.out_proj.weight torch.Size([512, 512])\n",
      "model.clip_model.transformer.resblocks.3.attn.out_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.3.ln_1.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.3.ln_1.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.3.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model.clip_model.transformer.resblocks.3.mlp.c_fc.bias torch.Size([2048])\n",
      "model.clip_model.transformer.resblocks.3.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model.clip_model.transformer.resblocks.3.mlp.c_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.3.ln_2.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.3.ln_2.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.4.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model.clip_model.transformer.resblocks.4.attn.in_proj_bias torch.Size([1536])\n",
      "model.clip_model.transformer.resblocks.4.attn.out_proj.weight torch.Size([512, 512])\n",
      "model.clip_model.transformer.resblocks.4.attn.out_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.4.ln_1.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.4.ln_1.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.4.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model.clip_model.transformer.resblocks.4.mlp.c_fc.bias torch.Size([2048])\n",
      "model.clip_model.transformer.resblocks.4.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model.clip_model.transformer.resblocks.4.mlp.c_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.4.ln_2.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.4.ln_2.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.5.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model.clip_model.transformer.resblocks.5.attn.in_proj_bias torch.Size([1536])\n",
      "model.clip_model.transformer.resblocks.5.attn.out_proj.weight torch.Size([512, 512])\n",
      "model.clip_model.transformer.resblocks.5.attn.out_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.5.ln_1.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.5.ln_1.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.5.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model.clip_model.transformer.resblocks.5.mlp.c_fc.bias torch.Size([2048])\n",
      "model.clip_model.transformer.resblocks.5.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model.clip_model.transformer.resblocks.5.mlp.c_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.5.ln_2.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.5.ln_2.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.6.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model.clip_model.transformer.resblocks.6.attn.in_proj_bias torch.Size([1536])\n",
      "model.clip_model.transformer.resblocks.6.attn.out_proj.weight torch.Size([512, 512])\n",
      "model.clip_model.transformer.resblocks.6.attn.out_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.6.ln_1.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.6.ln_1.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.6.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model.clip_model.transformer.resblocks.6.mlp.c_fc.bias torch.Size([2048])\n",
      "model.clip_model.transformer.resblocks.6.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model.clip_model.transformer.resblocks.6.mlp.c_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.6.ln_2.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.6.ln_2.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.7.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model.clip_model.transformer.resblocks.7.attn.in_proj_bias torch.Size([1536])\n",
      "model.clip_model.transformer.resblocks.7.attn.out_proj.weight torch.Size([512, 512])\n",
      "model.clip_model.transformer.resblocks.7.attn.out_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.7.ln_1.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.7.ln_1.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.7.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model.clip_model.transformer.resblocks.7.mlp.c_fc.bias torch.Size([2048])\n",
      "model.clip_model.transformer.resblocks.7.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model.clip_model.transformer.resblocks.7.mlp.c_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.7.ln_2.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.7.ln_2.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.8.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model.clip_model.transformer.resblocks.8.attn.in_proj_bias torch.Size([1536])\n",
      "model.clip_model.transformer.resblocks.8.attn.out_proj.weight torch.Size([512, 512])\n",
      "model.clip_model.transformer.resblocks.8.attn.out_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.8.ln_1.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.8.ln_1.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.8.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model.clip_model.transformer.resblocks.8.mlp.c_fc.bias torch.Size([2048])\n",
      "model.clip_model.transformer.resblocks.8.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model.clip_model.transformer.resblocks.8.mlp.c_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.8.ln_2.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.8.ln_2.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.9.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model.clip_model.transformer.resblocks.9.attn.in_proj_bias torch.Size([1536])\n",
      "model.clip_model.transformer.resblocks.9.attn.out_proj.weight torch.Size([512, 512])\n",
      "model.clip_model.transformer.resblocks.9.attn.out_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.9.ln_1.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.9.ln_1.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.9.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model.clip_model.transformer.resblocks.9.mlp.c_fc.bias torch.Size([2048])\n",
      "model.clip_model.transformer.resblocks.9.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model.clip_model.transformer.resblocks.9.mlp.c_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.9.ln_2.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.9.ln_2.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.10.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model.clip_model.transformer.resblocks.10.attn.in_proj_bias torch.Size([1536])\n",
      "model.clip_model.transformer.resblocks.10.attn.out_proj.weight torch.Size([512, 512])\n",
      "model.clip_model.transformer.resblocks.10.attn.out_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.10.ln_1.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.10.ln_1.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.10.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model.clip_model.transformer.resblocks.10.mlp.c_fc.bias torch.Size([2048])\n",
      "model.clip_model.transformer.resblocks.10.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model.clip_model.transformer.resblocks.10.mlp.c_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.10.ln_2.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.10.ln_2.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.11.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model.clip_model.transformer.resblocks.11.attn.in_proj_bias torch.Size([1536])\n",
      "model.clip_model.transformer.resblocks.11.attn.out_proj.weight torch.Size([512, 512])\n",
      "model.clip_model.transformer.resblocks.11.attn.out_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.11.ln_1.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.11.ln_1.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.11.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model.clip_model.transformer.resblocks.11.mlp.c_fc.bias torch.Size([2048])\n",
      "model.clip_model.transformer.resblocks.11.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model.clip_model.transformer.resblocks.11.mlp.c_proj.bias torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.11.ln_2.weight torch.Size([512])\n",
      "model.clip_model.transformer.resblocks.11.ln_2.bias torch.Size([512])\n",
      "model.clip_model.token_embedding.weight torch.Size([49408, 512])\n",
      "model.clip_model.ln_final.weight torch.Size([512])\n",
      "model.clip_model.ln_final.bias torch.Size([512])\n",
      "model.embed_text.weight torch.Size([512, 512])\n",
      "model.embed_text.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.0.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "model.seqTransEncoder.layers.0.self_attn.in_proj_bias torch.Size([1536])\n",
      "model.seqTransEncoder.layers.0.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "model.seqTransEncoder.layers.0.self_attn.out_proj.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.0.linear1.weight torch.Size([1024, 512])\n",
      "model.seqTransEncoder.layers.0.linear1.bias torch.Size([1024])\n",
      "model.seqTransEncoder.layers.0.linear2.weight torch.Size([512, 1024])\n",
      "model.seqTransEncoder.layers.0.linear2.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.0.norm1.weight torch.Size([512])\n",
      "model.seqTransEncoder.layers.0.norm1.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.0.norm2.weight torch.Size([512])\n",
      "model.seqTransEncoder.layers.0.norm2.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.1.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "model.seqTransEncoder.layers.1.self_attn.in_proj_bias torch.Size([1536])\n",
      "model.seqTransEncoder.layers.1.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "model.seqTransEncoder.layers.1.self_attn.out_proj.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.1.linear1.weight torch.Size([1024, 512])\n",
      "model.seqTransEncoder.layers.1.linear1.bias torch.Size([1024])\n",
      "model.seqTransEncoder.layers.1.linear2.weight torch.Size([512, 1024])\n",
      "model.seqTransEncoder.layers.1.linear2.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.1.norm1.weight torch.Size([512])\n",
      "model.seqTransEncoder.layers.1.norm1.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.1.norm2.weight torch.Size([512])\n",
      "model.seqTransEncoder.layers.1.norm2.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.2.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "model.seqTransEncoder.layers.2.self_attn.in_proj_bias torch.Size([1536])\n",
      "model.seqTransEncoder.layers.2.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "model.seqTransEncoder.layers.2.self_attn.out_proj.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.2.linear1.weight torch.Size([1024, 512])\n",
      "model.seqTransEncoder.layers.2.linear1.bias torch.Size([1024])\n",
      "model.seqTransEncoder.layers.2.linear2.weight torch.Size([512, 1024])\n",
      "model.seqTransEncoder.layers.2.linear2.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.2.norm1.weight torch.Size([512])\n",
      "model.seqTransEncoder.layers.2.norm1.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.2.norm2.weight torch.Size([512])\n",
      "model.seqTransEncoder.layers.2.norm2.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.3.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "model.seqTransEncoder.layers.3.self_attn.in_proj_bias torch.Size([1536])\n",
      "model.seqTransEncoder.layers.3.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "model.seqTransEncoder.layers.3.self_attn.out_proj.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.3.linear1.weight torch.Size([1024, 512])\n",
      "model.seqTransEncoder.layers.3.linear1.bias torch.Size([1024])\n",
      "model.seqTransEncoder.layers.3.linear2.weight torch.Size([512, 1024])\n",
      "model.seqTransEncoder.layers.3.linear2.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.3.norm1.weight torch.Size([512])\n",
      "model.seqTransEncoder.layers.3.norm1.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.3.norm2.weight torch.Size([512])\n",
      "model.seqTransEncoder.layers.3.norm2.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.4.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "model.seqTransEncoder.layers.4.self_attn.in_proj_bias torch.Size([1536])\n",
      "model.seqTransEncoder.layers.4.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "model.seqTransEncoder.layers.4.self_attn.out_proj.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.4.linear1.weight torch.Size([1024, 512])\n",
      "model.seqTransEncoder.layers.4.linear1.bias torch.Size([1024])\n",
      "model.seqTransEncoder.layers.4.linear2.weight torch.Size([512, 1024])\n",
      "model.seqTransEncoder.layers.4.linear2.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.4.norm1.weight torch.Size([512])\n",
      "model.seqTransEncoder.layers.4.norm1.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.4.norm2.weight torch.Size([512])\n",
      "model.seqTransEncoder.layers.4.norm2.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.5.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "model.seqTransEncoder.layers.5.self_attn.in_proj_bias torch.Size([1536])\n",
      "model.seqTransEncoder.layers.5.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "model.seqTransEncoder.layers.5.self_attn.out_proj.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.5.linear1.weight torch.Size([1024, 512])\n",
      "model.seqTransEncoder.layers.5.linear1.bias torch.Size([1024])\n",
      "model.seqTransEncoder.layers.5.linear2.weight torch.Size([512, 1024])\n",
      "model.seqTransEncoder.layers.5.linear2.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.5.norm1.weight torch.Size([512])\n",
      "model.seqTransEncoder.layers.5.norm1.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.5.norm2.weight torch.Size([512])\n",
      "model.seqTransEncoder.layers.5.norm2.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.6.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "model.seqTransEncoder.layers.6.self_attn.in_proj_bias torch.Size([1536])\n",
      "model.seqTransEncoder.layers.6.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "model.seqTransEncoder.layers.6.self_attn.out_proj.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.6.linear1.weight torch.Size([1024, 512])\n",
      "model.seqTransEncoder.layers.6.linear1.bias torch.Size([1024])\n",
      "model.seqTransEncoder.layers.6.linear2.weight torch.Size([512, 1024])\n",
      "model.seqTransEncoder.layers.6.linear2.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.6.norm1.weight torch.Size([512])\n",
      "model.seqTransEncoder.layers.6.norm1.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.6.norm2.weight torch.Size([512])\n",
      "model.seqTransEncoder.layers.6.norm2.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.7.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "model.seqTransEncoder.layers.7.self_attn.in_proj_bias torch.Size([1536])\n",
      "model.seqTransEncoder.layers.7.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "model.seqTransEncoder.layers.7.self_attn.out_proj.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.7.linear1.weight torch.Size([1024, 512])\n",
      "model.seqTransEncoder.layers.7.linear1.bias torch.Size([1024])\n",
      "model.seqTransEncoder.layers.7.linear2.weight torch.Size([512, 1024])\n",
      "model.seqTransEncoder.layers.7.linear2.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.7.norm1.weight torch.Size([512])\n",
      "model.seqTransEncoder.layers.7.norm1.bias torch.Size([512])\n",
      "model.seqTransEncoder.layers.7.norm2.weight torch.Size([512])\n",
      "model.seqTransEncoder.layers.7.norm2.bias torch.Size([512])\n",
      "model.output_process.poseFinal.weight torch.Size([263, 512])\n",
      "model.output_process.poseFinal.bias torch.Size([263])\n",
      "model_avg.input_process.poseEmbedding.weight torch.Size([512, 263])\n",
      "model_avg.input_process.poseEmbedding.bias torch.Size([512])\n",
      "model_avg.sequence_pos_encoder.pe torch.Size([5000, 1, 512])\n",
      "model_avg.embed_timestep.sequence_pos_encoder.pe torch.Size([5000, 1, 512])\n",
      "model_avg.embed_timestep.time_embed.0.weight torch.Size([512, 512])\n",
      "model_avg.embed_timestep.time_embed.0.bias torch.Size([512])\n",
      "model_avg.embed_timestep.time_embed.2.weight torch.Size([512, 512])\n",
      "model_avg.embed_timestep.time_embed.2.bias torch.Size([512])\n",
      "model_avg.clip_model.positional_embedding torch.Size([77, 512])\n",
      "model_avg.clip_model.text_projection torch.Size([512, 512])\n",
      "model_avg.clip_model.logit_scale torch.Size([])\n",
      "model_avg.clip_model.visual.class_embedding torch.Size([768])\n",
      "model_avg.clip_model.visual.positional_embedding torch.Size([50, 768])\n",
      "model_avg.clip_model.visual.proj torch.Size([768, 512])\n",
      "model_avg.clip_model.visual.conv1.weight torch.Size([768, 3, 32, 32])\n",
      "model_avg.clip_model.visual.ln_pre.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.ln_pre.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.0.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.0.attn.in_proj_bias torch.Size([2304])\n",
      "model_avg.clip_model.visual.transformer.resblocks.0.attn.out_proj.weight torch.Size([768, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.0.attn.out_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.0.ln_1.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.0.ln_1.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.0.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.0.mlp.c_fc.bias torch.Size([3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.0.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.0.mlp.c_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.0.ln_2.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.0.ln_2.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.1.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.1.attn.in_proj_bias torch.Size([2304])\n",
      "model_avg.clip_model.visual.transformer.resblocks.1.attn.out_proj.weight torch.Size([768, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.1.attn.out_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.1.ln_1.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.1.ln_1.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.1.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.1.mlp.c_fc.bias torch.Size([3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.1.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.1.mlp.c_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.1.ln_2.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.1.ln_2.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.2.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.2.attn.in_proj_bias torch.Size([2304])\n",
      "model_avg.clip_model.visual.transformer.resblocks.2.attn.out_proj.weight torch.Size([768, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.2.attn.out_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.2.ln_1.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.2.ln_1.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.2.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.2.mlp.c_fc.bias torch.Size([3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.2.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.2.mlp.c_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.2.ln_2.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.2.ln_2.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.3.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.3.attn.in_proj_bias torch.Size([2304])\n",
      "model_avg.clip_model.visual.transformer.resblocks.3.attn.out_proj.weight torch.Size([768, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.3.attn.out_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.3.ln_1.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.3.ln_1.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.3.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.3.mlp.c_fc.bias torch.Size([3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.3.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.3.mlp.c_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.3.ln_2.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.3.ln_2.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.4.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.4.attn.in_proj_bias torch.Size([2304])\n",
      "model_avg.clip_model.visual.transformer.resblocks.4.attn.out_proj.weight torch.Size([768, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.4.attn.out_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.4.ln_1.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.4.ln_1.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.4.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.4.mlp.c_fc.bias torch.Size([3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.4.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.4.mlp.c_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.4.ln_2.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.4.ln_2.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.5.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.5.attn.in_proj_bias torch.Size([2304])\n",
      "model_avg.clip_model.visual.transformer.resblocks.5.attn.out_proj.weight torch.Size([768, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.5.attn.out_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.5.ln_1.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.5.ln_1.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.5.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.5.mlp.c_fc.bias torch.Size([3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.5.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.5.mlp.c_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.5.ln_2.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.5.ln_2.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.6.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.6.attn.in_proj_bias torch.Size([2304])\n",
      "model_avg.clip_model.visual.transformer.resblocks.6.attn.out_proj.weight torch.Size([768, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.6.attn.out_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.6.ln_1.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.6.ln_1.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.6.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.6.mlp.c_fc.bias torch.Size([3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.6.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.6.mlp.c_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.6.ln_2.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.6.ln_2.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.7.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.7.attn.in_proj_bias torch.Size([2304])\n",
      "model_avg.clip_model.visual.transformer.resblocks.7.attn.out_proj.weight torch.Size([768, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.7.attn.out_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.7.ln_1.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.7.ln_1.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.7.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.7.mlp.c_fc.bias torch.Size([3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.7.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.7.mlp.c_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.7.ln_2.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.7.ln_2.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.8.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.8.attn.in_proj_bias torch.Size([2304])\n",
      "model_avg.clip_model.visual.transformer.resblocks.8.attn.out_proj.weight torch.Size([768, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.8.attn.out_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.8.ln_1.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.8.ln_1.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.8.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.8.mlp.c_fc.bias torch.Size([3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.8.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.8.mlp.c_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.8.ln_2.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.8.ln_2.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.9.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.9.attn.in_proj_bias torch.Size([2304])\n",
      "model_avg.clip_model.visual.transformer.resblocks.9.attn.out_proj.weight torch.Size([768, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.9.attn.out_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.9.ln_1.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.9.ln_1.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.9.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.9.mlp.c_fc.bias torch.Size([3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.9.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.9.mlp.c_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.9.ln_2.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.9.ln_2.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.10.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.10.attn.in_proj_bias torch.Size([2304])\n",
      "model_avg.clip_model.visual.transformer.resblocks.10.attn.out_proj.weight torch.Size([768, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.10.attn.out_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.10.ln_1.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.10.ln_1.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.10.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.10.mlp.c_fc.bias torch.Size([3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.10.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.10.mlp.c_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.10.ln_2.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.10.ln_2.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.11.attn.in_proj_weight torch.Size([2304, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.11.attn.in_proj_bias torch.Size([2304])\n",
      "model_avg.clip_model.visual.transformer.resblocks.11.attn.out_proj.weight torch.Size([768, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.11.attn.out_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.11.ln_1.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.11.ln_1.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.11.mlp.c_fc.weight torch.Size([3072, 768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.11.mlp.c_fc.bias torch.Size([3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.11.mlp.c_proj.weight torch.Size([768, 3072])\n",
      "model_avg.clip_model.visual.transformer.resblocks.11.mlp.c_proj.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.11.ln_2.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.transformer.resblocks.11.ln_2.bias torch.Size([768])\n",
      "model_avg.clip_model.visual.ln_post.weight torch.Size([768])\n",
      "model_avg.clip_model.visual.ln_post.bias torch.Size([768])\n",
      "model_avg.clip_model.transformer.resblocks.0.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model_avg.clip_model.transformer.resblocks.0.attn.in_proj_bias torch.Size([1536])\n",
      "model_avg.clip_model.transformer.resblocks.0.attn.out_proj.weight torch.Size([512, 512])\n",
      "model_avg.clip_model.transformer.resblocks.0.attn.out_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.0.ln_1.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.0.ln_1.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.0.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model_avg.clip_model.transformer.resblocks.0.mlp.c_fc.bias torch.Size([2048])\n",
      "model_avg.clip_model.transformer.resblocks.0.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model_avg.clip_model.transformer.resblocks.0.mlp.c_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.0.ln_2.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.0.ln_2.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.1.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model_avg.clip_model.transformer.resblocks.1.attn.in_proj_bias torch.Size([1536])\n",
      "model_avg.clip_model.transformer.resblocks.1.attn.out_proj.weight torch.Size([512, 512])\n",
      "model_avg.clip_model.transformer.resblocks.1.attn.out_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.1.ln_1.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.1.ln_1.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.1.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model_avg.clip_model.transformer.resblocks.1.mlp.c_fc.bias torch.Size([2048])\n",
      "model_avg.clip_model.transformer.resblocks.1.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model_avg.clip_model.transformer.resblocks.1.mlp.c_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.1.ln_2.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.1.ln_2.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.2.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model_avg.clip_model.transformer.resblocks.2.attn.in_proj_bias torch.Size([1536])\n",
      "model_avg.clip_model.transformer.resblocks.2.attn.out_proj.weight torch.Size([512, 512])\n",
      "model_avg.clip_model.transformer.resblocks.2.attn.out_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.2.ln_1.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.2.ln_1.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.2.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model_avg.clip_model.transformer.resblocks.2.mlp.c_fc.bias torch.Size([2048])\n",
      "model_avg.clip_model.transformer.resblocks.2.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model_avg.clip_model.transformer.resblocks.2.mlp.c_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.2.ln_2.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.2.ln_2.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.3.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model_avg.clip_model.transformer.resblocks.3.attn.in_proj_bias torch.Size([1536])\n",
      "model_avg.clip_model.transformer.resblocks.3.attn.out_proj.weight torch.Size([512, 512])\n",
      "model_avg.clip_model.transformer.resblocks.3.attn.out_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.3.ln_1.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.3.ln_1.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.3.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model_avg.clip_model.transformer.resblocks.3.mlp.c_fc.bias torch.Size([2048])\n",
      "model_avg.clip_model.transformer.resblocks.3.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model_avg.clip_model.transformer.resblocks.3.mlp.c_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.3.ln_2.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.3.ln_2.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.4.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model_avg.clip_model.transformer.resblocks.4.attn.in_proj_bias torch.Size([1536])\n",
      "model_avg.clip_model.transformer.resblocks.4.attn.out_proj.weight torch.Size([512, 512])\n",
      "model_avg.clip_model.transformer.resblocks.4.attn.out_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.4.ln_1.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.4.ln_1.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.4.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model_avg.clip_model.transformer.resblocks.4.mlp.c_fc.bias torch.Size([2048])\n",
      "model_avg.clip_model.transformer.resblocks.4.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model_avg.clip_model.transformer.resblocks.4.mlp.c_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.4.ln_2.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.4.ln_2.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.5.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model_avg.clip_model.transformer.resblocks.5.attn.in_proj_bias torch.Size([1536])\n",
      "model_avg.clip_model.transformer.resblocks.5.attn.out_proj.weight torch.Size([512, 512])\n",
      "model_avg.clip_model.transformer.resblocks.5.attn.out_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.5.ln_1.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.5.ln_1.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.5.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model_avg.clip_model.transformer.resblocks.5.mlp.c_fc.bias torch.Size([2048])\n",
      "model_avg.clip_model.transformer.resblocks.5.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model_avg.clip_model.transformer.resblocks.5.mlp.c_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.5.ln_2.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.5.ln_2.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.6.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model_avg.clip_model.transformer.resblocks.6.attn.in_proj_bias torch.Size([1536])\n",
      "model_avg.clip_model.transformer.resblocks.6.attn.out_proj.weight torch.Size([512, 512])\n",
      "model_avg.clip_model.transformer.resblocks.6.attn.out_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.6.ln_1.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.6.ln_1.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.6.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model_avg.clip_model.transformer.resblocks.6.mlp.c_fc.bias torch.Size([2048])\n",
      "model_avg.clip_model.transformer.resblocks.6.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model_avg.clip_model.transformer.resblocks.6.mlp.c_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.6.ln_2.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.6.ln_2.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.7.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model_avg.clip_model.transformer.resblocks.7.attn.in_proj_bias torch.Size([1536])\n",
      "model_avg.clip_model.transformer.resblocks.7.attn.out_proj.weight torch.Size([512, 512])\n",
      "model_avg.clip_model.transformer.resblocks.7.attn.out_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.7.ln_1.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.7.ln_1.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.7.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model_avg.clip_model.transformer.resblocks.7.mlp.c_fc.bias torch.Size([2048])\n",
      "model_avg.clip_model.transformer.resblocks.7.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model_avg.clip_model.transformer.resblocks.7.mlp.c_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.7.ln_2.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.7.ln_2.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.8.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model_avg.clip_model.transformer.resblocks.8.attn.in_proj_bias torch.Size([1536])\n",
      "model_avg.clip_model.transformer.resblocks.8.attn.out_proj.weight torch.Size([512, 512])\n",
      "model_avg.clip_model.transformer.resblocks.8.attn.out_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.8.ln_1.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.8.ln_1.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.8.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model_avg.clip_model.transformer.resblocks.8.mlp.c_fc.bias torch.Size([2048])\n",
      "model_avg.clip_model.transformer.resblocks.8.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model_avg.clip_model.transformer.resblocks.8.mlp.c_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.8.ln_2.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.8.ln_2.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.9.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model_avg.clip_model.transformer.resblocks.9.attn.in_proj_bias torch.Size([1536])\n",
      "model_avg.clip_model.transformer.resblocks.9.attn.out_proj.weight torch.Size([512, 512])\n",
      "model_avg.clip_model.transformer.resblocks.9.attn.out_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.9.ln_1.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.9.ln_1.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.9.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model_avg.clip_model.transformer.resblocks.9.mlp.c_fc.bias torch.Size([2048])\n",
      "model_avg.clip_model.transformer.resblocks.9.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model_avg.clip_model.transformer.resblocks.9.mlp.c_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.9.ln_2.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.9.ln_2.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.10.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model_avg.clip_model.transformer.resblocks.10.attn.in_proj_bias torch.Size([1536])\n",
      "model_avg.clip_model.transformer.resblocks.10.attn.out_proj.weight torch.Size([512, 512])\n",
      "model_avg.clip_model.transformer.resblocks.10.attn.out_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.10.ln_1.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.10.ln_1.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.10.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model_avg.clip_model.transformer.resblocks.10.mlp.c_fc.bias torch.Size([2048])\n",
      "model_avg.clip_model.transformer.resblocks.10.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model_avg.clip_model.transformer.resblocks.10.mlp.c_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.10.ln_2.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.10.ln_2.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.11.attn.in_proj_weight torch.Size([1536, 512])\n",
      "model_avg.clip_model.transformer.resblocks.11.attn.in_proj_bias torch.Size([1536])\n",
      "model_avg.clip_model.transformer.resblocks.11.attn.out_proj.weight torch.Size([512, 512])\n",
      "model_avg.clip_model.transformer.resblocks.11.attn.out_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.11.ln_1.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.11.ln_1.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.11.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "model_avg.clip_model.transformer.resblocks.11.mlp.c_fc.bias torch.Size([2048])\n",
      "model_avg.clip_model.transformer.resblocks.11.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "model_avg.clip_model.transformer.resblocks.11.mlp.c_proj.bias torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.11.ln_2.weight torch.Size([512])\n",
      "model_avg.clip_model.transformer.resblocks.11.ln_2.bias torch.Size([512])\n",
      "model_avg.clip_model.token_embedding.weight torch.Size([49408, 512])\n",
      "model_avg.clip_model.ln_final.weight torch.Size([512])\n",
      "model_avg.clip_model.ln_final.bias torch.Size([512])\n",
      "model_avg.embed_text.weight torch.Size([512, 512])\n",
      "model_avg.embed_text.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.0.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "model_avg.seqTransEncoder.layers.0.self_attn.in_proj_bias torch.Size([1536])\n",
      "model_avg.seqTransEncoder.layers.0.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "model_avg.seqTransEncoder.layers.0.self_attn.out_proj.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.0.linear1.weight torch.Size([1024, 512])\n",
      "model_avg.seqTransEncoder.layers.0.linear1.bias torch.Size([1024])\n",
      "model_avg.seqTransEncoder.layers.0.linear2.weight torch.Size([512, 1024])\n",
      "model_avg.seqTransEncoder.layers.0.linear2.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.0.norm1.weight torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.0.norm1.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.0.norm2.weight torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.0.norm2.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.1.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "model_avg.seqTransEncoder.layers.1.self_attn.in_proj_bias torch.Size([1536])\n",
      "model_avg.seqTransEncoder.layers.1.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "model_avg.seqTransEncoder.layers.1.self_attn.out_proj.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.1.linear1.weight torch.Size([1024, 512])\n",
      "model_avg.seqTransEncoder.layers.1.linear1.bias torch.Size([1024])\n",
      "model_avg.seqTransEncoder.layers.1.linear2.weight torch.Size([512, 1024])\n",
      "model_avg.seqTransEncoder.layers.1.linear2.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.1.norm1.weight torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.1.norm1.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.1.norm2.weight torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.1.norm2.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.2.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "model_avg.seqTransEncoder.layers.2.self_attn.in_proj_bias torch.Size([1536])\n",
      "model_avg.seqTransEncoder.layers.2.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "model_avg.seqTransEncoder.layers.2.self_attn.out_proj.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.2.linear1.weight torch.Size([1024, 512])\n",
      "model_avg.seqTransEncoder.layers.2.linear1.bias torch.Size([1024])\n",
      "model_avg.seqTransEncoder.layers.2.linear2.weight torch.Size([512, 1024])\n",
      "model_avg.seqTransEncoder.layers.2.linear2.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.2.norm1.weight torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.2.norm1.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.2.norm2.weight torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.2.norm2.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.3.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "model_avg.seqTransEncoder.layers.3.self_attn.in_proj_bias torch.Size([1536])\n",
      "model_avg.seqTransEncoder.layers.3.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "model_avg.seqTransEncoder.layers.3.self_attn.out_proj.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.3.linear1.weight torch.Size([1024, 512])\n",
      "model_avg.seqTransEncoder.layers.3.linear1.bias torch.Size([1024])\n",
      "model_avg.seqTransEncoder.layers.3.linear2.weight torch.Size([512, 1024])\n",
      "model_avg.seqTransEncoder.layers.3.linear2.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.3.norm1.weight torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.3.norm1.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.3.norm2.weight torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.3.norm2.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.4.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "model_avg.seqTransEncoder.layers.4.self_attn.in_proj_bias torch.Size([1536])\n",
      "model_avg.seqTransEncoder.layers.4.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "model_avg.seqTransEncoder.layers.4.self_attn.out_proj.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.4.linear1.weight torch.Size([1024, 512])\n",
      "model_avg.seqTransEncoder.layers.4.linear1.bias torch.Size([1024])\n",
      "model_avg.seqTransEncoder.layers.4.linear2.weight torch.Size([512, 1024])\n",
      "model_avg.seqTransEncoder.layers.4.linear2.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.4.norm1.weight torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.4.norm1.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.4.norm2.weight torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.4.norm2.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.5.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "model_avg.seqTransEncoder.layers.5.self_attn.in_proj_bias torch.Size([1536])\n",
      "model_avg.seqTransEncoder.layers.5.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "model_avg.seqTransEncoder.layers.5.self_attn.out_proj.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.5.linear1.weight torch.Size([1024, 512])\n",
      "model_avg.seqTransEncoder.layers.5.linear1.bias torch.Size([1024])\n",
      "model_avg.seqTransEncoder.layers.5.linear2.weight torch.Size([512, 1024])\n",
      "model_avg.seqTransEncoder.layers.5.linear2.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.5.norm1.weight torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.5.norm1.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.5.norm2.weight torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.5.norm2.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.6.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "model_avg.seqTransEncoder.layers.6.self_attn.in_proj_bias torch.Size([1536])\n",
      "model_avg.seqTransEncoder.layers.6.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "model_avg.seqTransEncoder.layers.6.self_attn.out_proj.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.6.linear1.weight torch.Size([1024, 512])\n",
      "model_avg.seqTransEncoder.layers.6.linear1.bias torch.Size([1024])\n",
      "model_avg.seqTransEncoder.layers.6.linear2.weight torch.Size([512, 1024])\n",
      "model_avg.seqTransEncoder.layers.6.linear2.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.6.norm1.weight torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.6.norm1.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.6.norm2.weight torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.6.norm2.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.7.self_attn.in_proj_weight torch.Size([1536, 512])\n",
      "model_avg.seqTransEncoder.layers.7.self_attn.in_proj_bias torch.Size([1536])\n",
      "model_avg.seqTransEncoder.layers.7.self_attn.out_proj.weight torch.Size([512, 512])\n",
      "model_avg.seqTransEncoder.layers.7.self_attn.out_proj.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.7.linear1.weight torch.Size([1024, 512])\n",
      "model_avg.seqTransEncoder.layers.7.linear1.bias torch.Size([1024])\n",
      "model_avg.seqTransEncoder.layers.7.linear2.weight torch.Size([512, 1024])\n",
      "model_avg.seqTransEncoder.layers.7.linear2.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.7.norm1.weight torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.7.norm1.bias torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.7.norm2.weight torch.Size([512])\n",
      "model_avg.seqTransEncoder.layers.7.norm2.bias torch.Size([512])\n",
      "model_avg.output_process.poseFinal.weight torch.Size([263, 512])\n",
      "model_avg.output_process.poseFinal.bias torch.Size([263])\n",
      "820\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(\n",
    "    # \"/home/mahatano/workspace/mdm/output/2025-08-29/22-36-41/train_mdm/checkpoints/model_step=000000060.ckpt\",\n",
    "    \"/home/mahatano/workspace/mdm/output/2025-08-29/23-01-04/train_mdm/checkpoints/model_step=000000010.ckpt\",\n",
    "    weights_only=False,\n",
    ")\n",
    "\n",
    "print(ckpt.keys())\n",
    "print(len(ckpt[\"state_dict\"].keys()))\n",
    "\n",
    "for k,v in ckpt[\"state_dict\"].items():\n",
    "    print(k, v.shape)\n",
    "print(len(ckpt[\"state_dict\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b24e0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
